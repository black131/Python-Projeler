# -*- coding: utf-8 -*-
"""Yinelemeli sinir ağı katmanı ile IMDB Verisi Örneği .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ks9RWZShTyu3d5quo5bSyFUv3zeH4EsJ
"""

import keras
from keras.models import Sequential
from keras.layers import Embedding
from keras.layers import SimpleRNN

#Bir RNN modeli
model=Sequential()
model.add(Embedding(1000,32))
model.add(SimpleRNN(32))
model.summary()

#Boyutlandırılmış RNN modeli
model=Sequential()
model.add(Embedding(1000,32))
model.add(SimpleRNN(32,return_sequences=True))
model.summary()

#Ardışık RNN modeli
model=Sequential()
model.add(Embedding(1000,32))
model.add(SimpleRNN(32,return_sequences=True))
model.add(SimpleRNN(32,return_sequences=True))
model.add(SimpleRNN(32,return_sequences=True))
model.add(SimpleRNN(32))
model.summary()

#IMDB veriseti hazırlanması
from keras.datasets import imdb
from keras.preprocessing import sequence

num_features=1000
maxlen=500
batch_size=32

print("Load data..")
(input_train,y_train),(input_test,y_test)=imdb.load_data(num_words=num_features)
print(len(input_train),"train sequence,",input_train.shape)
print(len(input_test),"test sequence",input_test.shape)

print("Pad sequences (samples x time)")
input_train=sequence.pad_sequences(input_train,maxlen=maxlen)
input_test=sequence.pad_sequences(input_test,maxlen=maxlen)
print(len(input_train),"train sequence,",input_train.shape)
print(len(input_test),"test sequence",input_test.shape)

#Embedding katmanı ve SimpleRNN katmanı
from keras.layers import Dense
from keras import layers

model=Sequential()
model.add(Embedding(num_features,32))
model.add(SimpleRNN(32))
model.add(Dense(1,activation="sigmoid"))
model.compile(optimizer="rmsprop",loss="binary_crossentropy",metrics=["acc"])
history=model.fit(input_train,y_train,
                  epochs=10,
                  batch_size=128,
                  validation_split=0.2)

#Sonuçların Çizdirilmesi
import matplotlib.pyplot as plt

acc=history.history["acc"]
val_acc=history.history["val_acc"]
loss=history.history["loss"]
val_loss=history.history["val_loss"]
epochs=range(1,len(acc)+1)

plt.plot(epochs,acc,"m*-",label="Eğitim Başarımı")
plt.plot(epochs,val_acc,"g*-",label="Doğrulama/Geçerleme başarımı")
plt.title('Eğitim ve Doğrulama için Başarım')
plt.legend()

plt.figure()

plt.plot(epochs,loss,"m*-",label="Eğitim Kayıbı")
plt.plot(epochs,val_loss,"g*-",label="Doğrulama/Geçerleme başarımı")
plt.title('Eğitim ve Doğrulama için Kayıp')
plt.legend()
plt.show()

print(acc,"Eğitim Başarımları")

#IMDB veriseti hazırlanması
from keras.datasets import imdb
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense


num_features=1000
maxlen=500
batch_size=32

print("Load data..")
(input_train,y_train),(input_test,y_test)=imdb.load_data(num_words=num_features)
print(len(input_train),"train sequence,Eğitim ")
print(len(input_test),"test sequence")

# Pad the sequences to a uniform length
input_train = sequence.pad_sequences(input_train, maxlen=maxlen)
input_test = sequence.pad_sequences(input_test, maxlen=maxlen)

model=Sequential()
model.add(Embedding(num_features,32))
model.add(SimpleRNN(32))
model.add(Dense(1,activation="sigmoid"))
model.compile(optimizer="rmsprop",loss="binary_crossentropy",metrics=["acc"])
history=model.fit(input_train,y_train,
                  epochs=10,
                  batch_size=128,
                  validation_split=0.2)